{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/github/shifty\n",
      "<function loglikelihood_fn at 0x7f5cc85f2950>\n"
     ]
    }
   ],
   "source": [
    "%cd /home/kpmurphy/github/shifty\n",
    "#from shifty.skax.logreg_flax import *\n",
    "from shifty.skax.skax import *\n",
    "\n",
    "print(logprior_fn) # check that one of the symbols is defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n",
    "# https://github.com/tensorflow/probability/issues/1523\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import scipy.stats\n",
    "import einops\n",
    "from functools import partial\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "import chex\n",
    "import typing\n",
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit\n",
    "from jax import lax, random, numpy as jnp\n",
    "import jax.scipy as jsp\n",
    "\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "\n",
    "import jaxopt\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import distrax\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cpu_count = os.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Run jax on multiple CPU cores\n",
    "# https://github.com/google/jax/issues/5506\n",
    "# https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\n",
    "import os \n",
    "#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=90'\n",
    "\n",
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shifty.skax.logreg_flax import *\n",
    "\n",
    "print(loglikelihood_fn) # check that one of the symbols is defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "@chex.dataclass\n",
    "class GenParams:\n",
    "    nclasses: int\n",
    "    nfeatures: int\n",
    "    prior: chex.Array\n",
    "    mus: chex.Array # (C,D)\n",
    "    Sigmas: chex.Array #(C,D,D)\n",
    "\n",
    "def make_params(key, nclasses, nfeatures, scale_factor=1):\n",
    "    mus = jr.normal(key, (nclasses, nfeatures)) # (C,D)\n",
    "    # shared covariance -> linearly separable\n",
    "    #Sigma = scale_factor * jnp.eye(nfeatures)\n",
    "    #Sigmas = jnp.array([Sigma for _ in range(nclasses)]) # (C,D,D)\n",
    "    # diagonal covariance -> nonlinear decision boundaries\n",
    "    sigmas = jr.uniform(key, shape=(nclasses, nfeatures), minval=0.5, maxval=5)\n",
    "    Sigmas = jnp.array([scale_factor*jnp.diag(sigmas[y]) for y in range(nclasses)])\n",
    "    prior = jnp.ones(nclasses)/nclasses\n",
    "    return GenParams(nclasses=nclasses, nfeatures=nfeatures, prior=prior, mus=mus, Sigmas=Sigmas)\n",
    "\n",
    "def sample_data(key, params, nsamples):\n",
    "    y = jr.categorical(key, logits=jnp.log(params.prior), shape=(nsamples,))\n",
    "    X = jr.multivariate_normal(key, params.mus[y], params.Sigmas[y])\n",
    "    return X, y\n",
    "\n",
    "def predict_bayes(X, params):\n",
    "    def lik_fn(y):\n",
    "        return   jsp.stats.multivariate_normal.pdf(X, params.mus[y], params.Sigmas[y])\n",
    "    liks = vmap(lik_fn)(jnp.arange(params.nclasses)) # liks(k,n)=p(X(n,:) | y=k)\n",
    "    joint = jnp.einsum('kn,k -> nk', liks, params.prior) # joint(n,k) = liks(k,n) * prior(k)\n",
    "    norm = joint.sum(axis=1) # norm(n)  = sum_k joint(n,k) = p(X(n,:)\n",
    "    post = joint / jnp.expand_dims(norm, axis=1) # post(n,k) = p(y = k | xn)\n",
    "    return post\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26700002\n",
      "0.303\n"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "key, subkey = jr.split(key)\n",
    "params = make_params(subkey, nclasses=4, nfeatures=10, scale_factor=5)\n",
    "key, subkey = jr.split(key)\n",
    "Xtrain, ytrain = sample_data(subkey, params, nsamples=1000)\n",
    "key, subkey = jr.split(key)\n",
    "Xtest, ytest = sample_data(subkey, params, nsamples=1000)\n",
    "\n",
    "yprobs_train_bayes = predict_bayes(Xtrain, params)\n",
    "yprobs_test_bayes = predict_bayes(Xtest, params)\n",
    "\n",
    "ypred_train_bayes = jnp.argmax(yprobs_train_bayes, axis=1)\n",
    "error_rate_train_bayes = jnp.sum(ypred_train_bayes != ytrain) / len(ytrain)\n",
    "print(error_rate_train_bayes)\n",
    "\n",
    "ypred_test_bayes = jnp.argmax(yprobs_test_bayes, axis=1)\n",
    "error_rate_test_bayes = jnp.sum(ypred_test_bayes != ytest) / len(ytest)\n",
    "print(error_rate_test_bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 47.790\n",
      "epoch 10, train loss 43.175\n",
      "0.45600003\n",
      "0.453\n"
     ]
    }
   ],
   "source": [
    "\n",
    "network = MLPNetwork((nclasses,)) # no hidden layers == logistic regression\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, optimizer = \"adam+warmup\", \n",
    "        batch_size=32, num_epochs=20, print_every=10)  \n",
    "mlp.fit(Xtrain, ytrain)\n",
    "\n",
    "yprobs_train_mlp = np.array(mlp.predict(Xtrain))\n",
    "yprobs_test_mlp = np.array(mlp.predict(Xtest))\n",
    "\n",
    "ypred_train_mlp = jnp.argmax(yprobs_train_mlp, axis=1)\n",
    "error_rate_train_mlp = jnp.sum(ypred_train_mlp != ytrain) / len(ytrain)\n",
    "print(error_rate_train_mlp)\n",
    "\n",
    "ypred_test_mlp = jnp.argmax(yprobs_test_mlp, axis=1)\n",
    "error_rate_test_mlp = jnp.sum(ypred_test_mlp != ytest) / len(ytest)\n",
    "print(error_rate_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 96.741\n",
      "epoch 10, train loss 84.876\n",
      "epoch 20, train loss 82.873\n",
      "epoch 30, train loss 79.111\n",
      "epoch 40, train loss 75.705\n",
      "0.26200002\n",
      "0.42200002\n"
     ]
    }
   ],
   "source": [
    "network = MLPNetwork((10, 10, nclasses,)) # num hidden units per layer\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, optimizer = \"adam+warmup\", \n",
    "        batch_size=32, num_epochs=50, print_every=10)  \n",
    "mlp.fit(Xtrain, ytrain)\n",
    "\n",
    "yprobs_train_mlp = np.array(mlp.predict(Xtrain))\n",
    "yprobs_test_mlp = np.array(mlp.predict(Xtest))\n",
    "\n",
    "ypred_train_mlp = jnp.argmax(yprobs_train_mlp, axis=1)\n",
    "error_rate_train_mlp = jnp.sum(ypred_train_mlp != ytrain) / len(ytrain)\n",
    "print(error_rate_train_mlp)\n",
    "\n",
    "ypred_test_mlp = jnp.argmax(yprobs_test_mlp, axis=1)\n",
    "error_rate_test_mlp = jnp.sum(ypred_test_mlp != ytest) / len(ytest)\n",
    "print(error_rate_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 98.647\n",
      "epoch 10, train loss 89.701\n",
      "epoch 20, train loss 85.274\n",
      "epoch 30, train loss 82.865\n",
      "epoch 40, train loss 81.472\n",
      "0.36\n",
      "0.41900003\n"
     ]
    }
   ],
   "source": [
    "network = MLPNetwork((10, 10, nclasses,)) # num hidden units per layer\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, optimizer =  optax.adam(1e-3), \n",
    "        batch_size=32, num_epochs=50, print_every=10)  \n",
    "mlp.fit(Xtrain, ytrain)\n",
    "\n",
    "yprobs_train_mlp = np.array(mlp.predict(Xtrain))\n",
    "yprobs_test_mlp = np.array(mlp.predict(Xtest))\n",
    "\n",
    "ypred_train_mlp = jnp.argmax(yprobs_train_mlp, axis=1)\n",
    "error_rate_train_mlp = jnp.sum(ypred_train_mlp != ytrain) / len(ytrain)\n",
    "print(error_rate_train_mlp)\n",
    "\n",
    "ypred_test_mlp = jnp.argmax(yprobs_test_mlp, axis=1)\n",
    "error_rate_test_mlp = jnp.sum(ypred_test_mlp != ytest) / len(ytest)\n",
    "print(error_rate_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 59.498\n",
      "epoch 10, train loss 43.852\n",
      "0.37500003\n",
      "0.409\n"
     ]
    }
   ],
   "source": [
    "network = MLPNetwork((5, nclasses,)) # 5 hidden units in first layer\n",
    "#network = MLPNetwork((nclasses,)) # no hidden layers == logistic regression\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, optimizer = \"adam+warmup\", \n",
    "        batch_size=32, num_epochs=20, print_every=10)  \n",
    "mlp.fit(Xtrain, ytrain)\n",
    "\n",
    "yprobs_train_mlp = np.array(mlp.predict(Xtrain))\n",
    "yprobs_test_mlp = np.array(mlp.predict(Xtest))\n",
    "\n",
    "ypred_train_mlp = jnp.argmax(yprobs_train_mlp, axis=1)\n",
    "error_rate_train_mlp = jnp.sum(ypred_train_mlp != ytrain) / len(ytrain)\n",
    "print(error_rate_train_mlp)\n",
    "\n",
    "ypred_test_mlp = jnp.argmax(yprobs_test_mlp, axis=1)\n",
    "error_rate_test_mlp = jnp.sum(ypred_test_mlp != ytest) / len(ytest)\n",
    "print(error_rate_test_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 64.890\n",
      "epoch 10, train loss 56.047\n",
      "epoch 20, train loss 52.202\n",
      "epoch 30, train loss 49.440\n",
      "epoch 40, train loss 47.341\n",
      "0.42700002\n",
      "0.44200003\n"
     ]
    }
   ],
   "source": [
    "network = MLPNetwork((5, nclasses,)) # 5 hidden units in first layer\n",
    "mlp = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, optimizer = optax.adam(1e-3), \n",
    "        batch_size=32, num_epochs=50, print_every=10)  \n",
    "mlp.fit(Xtrain, ytrain)\n",
    "\n",
    "yprobs_train_mlp = np.array(mlp.predict(Xtrain))\n",
    "yprobs_test_mlp = np.array(mlp.predict(Xtest))\n",
    "\n",
    "ypred_train_mlp = jnp.argmax(yprobs_train_mlp, axis=1)\n",
    "error_rate_train_mlp = jnp.sum(ypred_train_mlp != ytrain) / len(ytrain)\n",
    "print(error_rate_train_mlp)\n",
    "\n",
    "ypred_test_mlp = jnp.argmax(yprobs_test_mlp, axis=1)\n",
    "error_rate_test_mlp = jnp.sum(ypred_test_mlp != ytest) / len(ytest)\n",
    "print(error_rate_test_mlp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1aafb1a5b8a6c5cc9d9564fe8ce376ad7cec1976d94f450e8b79a35770c931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
