{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/kpmurphy/github/shifty/shifty/skax'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/github/shifty/shifty\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/github/shifty\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function loglikelihood_fn at 0x7f89f429b5b0>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from shifty.skax.logreg_flax import *\n",
    "from shifty.skax.skax import *\n",
    "\n",
    "print(loglikelihood_fn) # check that one of the symbols is defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n",
    "# https://github.com/tensorflow/probability/issues/1523\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import scipy.stats\n",
    "import einops\n",
    "from functools import partial\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "import chex\n",
    "import typing\n",
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit\n",
    "from jax import lax, random, numpy as jnp\n",
    "#import jax.debug\n",
    "\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "\n",
    "import jaxopt\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import distrax\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "[CpuDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cpu_count = os.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Run jax on multiple CPU cores\n",
    "# https://github.com/google/jax/issues/5506\n",
    "# https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\n",
    "import os \n",
    "#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=90'\n",
    "\n",
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shifty.skax.logreg_flax import *\n",
    "\n",
    "print(loglikelihood_fn) # check that one of the symbols is defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def make_test_data():\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    X = iris[\"data\"]\n",
    "    #y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0'\n",
    "    y = iris[\"target\"]\n",
    "    nclasses = len(np.unique(y)) # \n",
    "    ndata, ndim = X.shape  # 150, 4\n",
    "    key = jr.PRNGKey(0)\n",
    "    noise = jr.normal(key, (ndata, ndim)) * 2.0\n",
    "    X = X + noise # add noise to make the classes less separable\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X = np.array(X)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X))\n",
    "#print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_test_data()\n",
    "# We set C to a large number to turn off regularization.\n",
    "# We don't fit the bias term to simplify the comparison below.\n",
    "l2reg = 1e-2\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=1/l2reg, fit_intercept=True)\n",
    "log_reg.fit(X, y)\n",
    "W_mle = log_reg.coef_ # (nclasses, ndim)\n",
    "b_mle = log_reg.intercept_ # (nclasses,)\n",
    "true_probs = log_reg.predict_proba(X)\n",
    "nclasses, ndim = W_mle.shape\n",
    "ntrain = X.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  [0.851 0.14  0.009]\n",
      "truth:  [0.858 0.134 0.008]\n"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "nclasses, ndim = W_mle.shape\n",
    "print([nclasses, ndim])\n",
    "key = jr.PRNGKey(0)\n",
    "model = LogReg(key,  nclasses, W_init=W_mle.T, b_init=b_mle, l2reg=l2reg)\n",
    "probs = np.array(model.predict(X))\n",
    "\n",
    "model = LogReg(key, nclasses)\n",
    "model.fit(X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "\n",
    "print('pred: ', probs[0])\n",
    "print('truth: ', true_probs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_method(optimizer, name=None, batch_size=ntrain, max_iter=500):\n",
    "    key = jr.PRNGKey(0)\n",
    "    model = LogReg(key, nclasses, max_iter=max_iter, l2reg=l2reg, optimizer=optimizer, batch_size=batch_size)  \n",
    "    model.fit(X, y)\n",
    "    probs = np.array(model.predict(X))\n",
    "    print('method {:s}, max deviation from true probs {:.3f}'.format(name, np.max(true_probs - probs)))\n",
    "    print('truth: ', true_probs[0])\n",
    "    print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method LBFGS, max deviation from true probs 0.011\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.855 0.137 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_method(\"LBFGS\", \"LBFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method adam 0.01, bs=N, max deviation from true probs 0.163\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.801 0.169 0.03 ]\n"
     ]
    }
   ],
   "source": [
    "compare_method(optax.adam(0.01), name=\"adam 0.01, bs=N\", batch_size=ntrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method adam 0.01, bs=32, max deviation from true probs 0.235\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.771 0.185 0.044]\n"
     ]
    }
   ],
   "source": [
    "compare_method(optax.adam(0.01), name=\"adam 0.01, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method polyak, bs=32, max deviation from true probs 0.645\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.735 0.256 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_method(\"polyak\", name=\"polyak, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method armijo, bs=32, max deviation from true probs 0.251\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.757 0.199 0.044]\n"
     ]
    }
   ],
   "source": [
    "compare_method(\"armijo\", name=\"armijo, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.617e-01 1.380e-01 2.624e-04]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit_classifier_sklearn(key, X, Y):\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', LogisticRegression(random_state=0, max_iter=100, C=1e5))])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "clf = fit_classifier_sklearn(key, X, y)\n",
    "\n",
    "true_probs = clf.predict_proba(X)\n",
    "print(true_probs[0])\n",
    "print(np.sum(true_probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max deviation from true probs 0.006\n",
      "truth:  [8.617e-01 1.380e-01 2.624e-04]\n",
      "pred:  [8.606e-01 1.391e-01 2.525e-04]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_classifier_skax(key, X, Y):\n",
    "    ndim = X.shape[1]\n",
    "    nclasses  = len(np.unique(Y))\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', LogReg(key, nclasses, max_iter=500, l2reg=1e-5))])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "model = fit_classifier_skax(key, X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "print('max deviation from true probs {:.3f}'.format(np.max(true_probs - probs)))\n",
    "print('truth: ', true_probs[0])\n",
    "print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skax logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_test_data()\n",
    "l2reg = 1e-2\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=1/l2reg, fit_intercept=True)\n",
    "log_reg.fit(X, y)\n",
    "true_probs = log_reg.predict_proba(X)\n",
    "ninputs = X.shape[1]\n",
    "ntrain = X.shape[0]\n",
    "nclasses = len(np.unique(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_skax_method(optimizer, name=None, batch_size=ntrain, max_iter=500):\n",
    "    key = jr.PRNGKey(0)\n",
    "    network = LogRegNetwork(nclasses = nclasses)\n",
    "    model = NeuralNetClassifier(network, key, nclasses, l2reg=l2reg, optimizer = optimizer, batch_size=batch_size, max_iter=max_iter) \n",
    "    model.fit(X, y)\n",
    "    probs = np.array(model.predict(X))\n",
    "    print('method {:s}, max deviation from true probs {:.3f}'.format(name, np.max(true_probs - probs)))\n",
    "    print('truth: ', true_probs[0])\n",
    "    print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ninputs  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method LBFGS, max deviation from true probs 0.011\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.855 0.137 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_skax_method(\"LBFGS\", \"LBFGS\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method armijo, bs=32, max deviation from true probs 0.251\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.757 0.199 0.044]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_skax_method(\"armijo\", name=\"armijo, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets with no hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.617e-01 1.380e-01 2.624e-04]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "def fit_classifier_sklearn(key, X, Y):\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', LogisticRegression(random_state=0, max_iter=100, C=1e5))])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "clf = fit_classifier_sklearn(key, X, y)\n",
    "\n",
    "true_probs_pipeline = clf.predict_proba(X)\n",
    "print(true_probs_pipeline[0])\n",
    "print(np.sum(true_probs_pipeline[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max deviation from true probs 0.004\n",
      "truth:  [8.617e-01 1.380e-01 2.624e-04]\n",
      "pred:  [8.622e-01 1.375e-01 2.553e-04]\n"
     ]
    }
   ],
   "source": [
    "def fit_classifier_skax_mlp(key, X, Y):\n",
    "    ndim = X.shape[1]\n",
    "    nclasses  = len(np.unique(Y))\n",
    "    network = MLPNetwork((nclasses,))\n",
    "    model = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, max_iter=500)\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', model)])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "model = fit_classifier_skax_mlp(key, X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "print('max deviation from true probs {:.3f}'.format(np.max(true_probs_pipeline - probs)))\n",
    "print('truth: ', true_probs_pipeline[0])\n",
    "print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ninputs  4\n",
      "method LBFGS, max deviation from true probs 0.011\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.855 0.137 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_mlp_method(\"LBFGS\", \"LBFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ninputs  4\n",
      "method armijo, bs=32, max deviation from true probs 0.251\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.757 0.199 0.044]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_skax_method(\"armijo\", name=\"armijo, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets with hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max deviation from true probs 0.934\n",
      "truth:  [8.617e-01 1.380e-01 2.624e-04]\n",
      "pred:  [1.00e+00 8.77e-26 0.00e+00]\n"
     ]
    }
   ],
   "source": [
    "def fit_classifier_skax_mlp(key, X, Y):\n",
    "    ndim = X.shape[1]\n",
    "    nclasses  = len(np.unique(Y))\n",
    "    network = MLPNetwork((5, nclasses,))\n",
    "    model = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, max_iter=500)\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', model)])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "model = fit_classifier_skax_mlp(key, X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "print('max deviation from true probs {:.3f}'.format(np.max(true_probs_pipeline - probs)))\n",
    "print('truth: ', true_probs[0])\n",
    "print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1aafb1a5b8a6c5cc9d9564fe8ce376ad7cec1976d94f450e8b79a35770c931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
