{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/github/shifty\n",
      "<function loglikelihood_fn at 0x7f5cc85f2950>\n"
     ]
    }
   ],
   "source": [
    "%cd /home/kpmurphy/github/shifty\n",
    "#from shifty.skax.logreg_flax import *\n",
    "from shifty.skax.skax import *\n",
    "\n",
    "print(loglikelihood_fn) # check that one of the symbols is defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silence WARNING:root:The use of `check_types` is deprecated and does not have any effect.\n",
    "# https://github.com/tensorflow/probability/issues/1523\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger()\n",
    "\n",
    "\n",
    "class CheckTypesFilter(logging.Filter):\n",
    "    def filter(self, record):\n",
    "        return \"check_types\" not in record.getMessage()\n",
    "\n",
    "\n",
    "logger.addFilter(CheckTypesFilter())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=3)\n",
    "import scipy.stats\n",
    "import einops\n",
    "from functools import partial\n",
    "\n",
    "from functools import partial\n",
    "from collections import namedtuple\n",
    "import itertools\n",
    "from itertools import repeat\n",
    "from time import time\n",
    "import chex\n",
    "import typing\n",
    "from typing import Any, Callable, Sequence\n",
    "\n",
    "import jax\n",
    "import jax.random as jr\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, grad, jit\n",
    "from jax import lax, random, numpy as jnp\n",
    "#import jax.debug\n",
    "\n",
    "from flax.core import freeze, unfreeze\n",
    "from flax import linen as nn\n",
    "import flax\n",
    "\n",
    "import jaxopt\n",
    "import optax\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import distrax\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfd = tfp.distributions\n",
    "from tensorflow_probability.substrates.jax.distributions import MultivariateNormalFullCovariance as MVN\n",
    "\n",
    "#jax.config.update(\"jax_enable_x64\", False)\n",
    "\n",
    "\n",
    "import sklearn.datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n",
      "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cpu_count = os.cpu_count()\n",
    "print(cpu_count)\n",
    "\n",
    "# Run jax on multiple CPU cores\n",
    "# https://github.com/google/jax/issues/5506\n",
    "# https://stackoverflow.com/questions/72328521/jax-pmap-with-multi-core-cpu\n",
    "import os \n",
    "#os.environ[\"XLA_FLAGS\"] = '--xla_force_host_platform_device_count=90'\n",
    "\n",
    "import jax\n",
    "print(jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shifty.skax.logreg_flax import *\n",
    "\n",
    "print(loglikelihood_fn) # check that one of the symbols is defiend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 4)\n",
      "(150,)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "def make_test_data(key):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    X = iris[\"data\"]\n",
    "    #y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0'\n",
    "    y = iris[\"target\"]\n",
    "    nclasses = len(np.unique(y)) # \n",
    "    ndata, ndim = X.shape  # 150, 4\n",
    "    key = jr.PRNGKey(0)\n",
    "    noise = jr.normal(key, (ndata, ndim)) * 2.0\n",
    "    X = X + noise # add noise to make the classes less separable\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X = np.array(X)\n",
    "    perm = jr.permutation(key, ndata)\n",
    "    X, y = X[perm], y[perm]\n",
    "    return X, y\n",
    "\n",
    "key = jr.PRNGKey(0)\n",
    "X,y = make_test_data(key)\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(type(X))\n",
    "#print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(10, 1), (10,)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-08 17:25:41.846184: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/lib\n",
      "2022-10-08 17:25:41.858670: W tensorflow/stream_executor/cuda/cuda_driver.cc:263] failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1)\n",
      "[[2]\n",
      " [1]\n",
      " [0]\n",
      " [0]\n",
      " [7]]\n",
      "(5, 1)\n",
      "[[6]\n",
      " [5]\n",
      " [7]\n",
      " [0]\n",
      " [4]]\n",
      "(5, 1)\n",
      "[[0]\n",
      " [0]\n",
      " [3]\n",
      " [9]\n",
      " [8]]\n",
      "(5, 1)\n",
      "[[6]\n",
      " [0]\n",
      " [2]\n",
      " [7]\n",
      " [3]]\n",
      "(5, 1)\n",
      "[[3]\n",
      " [5]\n",
      " [0]\n",
      " [9]\n",
      " [5]]\n",
      "(5, 1)\n",
      "[[2]\n",
      " [7]\n",
      " [6]\n",
      " [1]\n",
      " [8]]\n",
      "(5, 1)\n",
      "[[2]\n",
      " [6]\n",
      " [7]\n",
      " [2]\n",
      " [9]]\n",
      "(5, 1)\n",
      "[[8]\n",
      " [4]\n",
      " [8]\n",
      " [7]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "X = np.reshape(np.arange(10), (10,1))\n",
    "y = np.arange(10)\n",
    "print([X.shape, y.shape])\n",
    "iterator  = make_data_iterator(X, y, 5)\n",
    "for i in range(8):\n",
    "    batch = next(iterator)\n",
    "    print(batch['X'].shape)\n",
    "    print(batch['X'])\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = make_test_data()\n",
    "# We set C to a large number to turn off regularization.\n",
    "# We don't fit the bias term to simplify the comparison below.\n",
    "l2reg = 1e-2\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=1/l2reg, fit_intercept=True)\n",
    "log_reg.fit(X, y)\n",
    "W_mle = log_reg.coef_ # (nclasses, ndim)\n",
    "b_mle = log_reg.intercept_ # (nclasses,)\n",
    "true_probs = log_reg.predict_proba(X)\n",
    "nclasses, ndim = W_mle.shape\n",
    "ntrain = X.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pred:  [0.851 0.14  0.009]\n",
      "truth:  [0.858 0.134 0.008]\n"
     ]
    }
   ],
   "source": [
    "key = jr.PRNGKey(0)\n",
    "nclasses, ndim = W_mle.shape\n",
    "print([nclasses, ndim])\n",
    "key = jr.PRNGKey(0)\n",
    "model = LogReg(key,  nclasses, W_init=W_mle.T, b_init=b_mle, l2reg=l2reg)\n",
    "probs = np.array(model.predict(X))\n",
    "\n",
    "model = LogReg(key, nclasses)\n",
    "model.fit(X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "\n",
    "print('pred: ', probs[0])\n",
    "print('truth: ', true_probs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_method(optimizer, name=None, batch_size=ntrain, max_iter=500):\n",
    "    key = jr.PRNGKey(0)\n",
    "    model = LogReg(key, nclasses, max_iter=max_iter, l2reg=l2reg, optimizer=optimizer, batch_size=batch_size)  \n",
    "    model.fit(X, y)\n",
    "    probs = np.array(model.predict(X))\n",
    "    print('method {:s}, max deviation from true probs {:.3f}'.format(name, np.max(true_probs - probs)))\n",
    "    print('truth: ', true_probs[0])\n",
    "    print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method LBFGS, max deviation from true probs 0.011\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.855 0.137 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_method(\"LBFGS\", \"LBFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method adam 0.01, bs=N, max deviation from true probs 0.163\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.801 0.169 0.03 ]\n"
     ]
    }
   ],
   "source": [
    "compare_method(optax.adam(0.01), name=\"adam 0.01, bs=N\", batch_size=ntrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method adam 0.01, bs=32, max deviation from true probs 0.235\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.771 0.185 0.044]\n"
     ]
    }
   ],
   "source": [
    "compare_method(optax.adam(0.01), name=\"adam 0.01, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method polyak, bs=32, max deviation from true probs 0.645\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.735 0.256 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_method(\"polyak\", name=\"polyak, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method armijo, bs=32, max deviation from true probs 0.251\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.757 0.199 0.044]\n"
     ]
    }
   ],
   "source": [
    "compare_method(\"armijo\", name=\"armijo, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logreg pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.617e-01 1.380e-01 2.624e-04]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def fit_classifier_sklearn(key, X, Y):\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', LogisticRegression(random_state=0, max_iter=100, C=1e5))])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "clf = fit_classifier_sklearn(key, X, y)\n",
    "\n",
    "true_probs = clf.predict_proba(X)\n",
    "print(true_probs[0])\n",
    "print(np.sum(true_probs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max deviation from true probs 0.006\n",
      "truth:  [8.617e-01 1.380e-01 2.624e-04]\n",
      "pred:  [8.606e-01 1.391e-01 2.525e-04]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fit_classifier_skax(key, X, Y):\n",
    "    ndim = X.shape[1]\n",
    "    nclasses  = len(np.unique(Y))\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', LogReg(key, nclasses, max_iter=500, l2reg=1e-5))])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "model = fit_classifier_skax(key, X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "print('max deviation from true probs {:.3f}'.format(np.max(true_probs - probs)))\n",
    "print('truth: ', true_probs[0])\n",
    "print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Skax logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_probs(probs):\n",
    "    str = ['{:0.3f}'.format(p) for p in probs]\n",
    "    print(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_iris_data(key):\n",
    "    iris = sklearn.datasets.load_iris()\n",
    "    X = iris[\"data\"]\n",
    "    #y = (iris[\"target\"] == 2).astype(np.int)  # 1 if Iris-Virginica, else 0'\n",
    "    y = iris[\"target\"]\n",
    "    nclasses = len(np.unique(y)) # \n",
    "    ndata, ndim = X.shape  # 150, 4\n",
    "    key = jr.PRNGKey(0)\n",
    "    noise = jr.normal(key, (ndata, ndim)) * 2.0\n",
    "    X = X + noise # add noise to make the classes less separable\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "    X = np.array(X)\n",
    "    perm = jr.permutation(key, ndata)\n",
    "    X, y = X[perm], y[perm]\n",
    "    return X, y\n",
    "\n",
    "key = jr.PRNGKey(0)\n",
    "X,y = make_iris_data(key)\n",
    "\n",
    "ninputs = X.shape[1]\n",
    "ntrain = X.shape[0]\n",
    "nclasses = len(np.unique(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500, 20, 10]\n"
     ]
    }
   ],
   "source": [
    "def make_random_data(seed):\n",
    "    X, y = sklearn.datasets.make_classification(n_samples=500, n_features=20,  n_informative=10, n_redundant=5, n_repeated=0,\n",
    "    n_classes=10, n_clusters_per_class=1, weights=None, flip_y=0.01, class_sep=0.5,\n",
    "    hypercube=True, shift=0.0, scale=1.0, shuffle=True, random_state=seed)\n",
    "    return X, y\n",
    "\n",
    "X,y = make_random_data(0)\n",
    "nfeatures = X.shape[1]\n",
    "ntrain = X.shape[0]\n",
    "nclasses = len(np.unique(y))\n",
    "print([ntrain, nfeatures, nclasses])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "l2reg = 1e-2\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", C=1/l2reg)\n",
    "log_reg.fit(X, y)\n",
    "true_probs = log_reg.predict_proba(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0.213', '0.030', '0.100', '0.043', '0.017', '0.049', '0.065', '0.085', '0.002', '0.396']\n"
     ]
    }
   ],
   "source": [
    "classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()), \n",
    "            ('logreg', LogisticRegression(random_state=0, max_iter=100, C=1/l2reg))])\n",
    "classifier.fit(X, y)\n",
    "true_probs = classifier.predict_proba(X)\n",
    "print_probs(true_probs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_skax_method(optimizer, name=None, batch_size=ntrain, max_iter=500, num_epochs=5):\n",
    "    key = jr.PRNGKey(0)\n",
    "    network = LogRegNetwork(nclasses = nclasses)\n",
    "    model = NeuralNetClassifier(network, key, nclasses, l2reg=l2reg, optimizer = optimizer, batch_size=batch_size,\n",
    "                max_iter=max_iter, num_epochs=num_epochs, print_every=5) \n",
    "    model.fit(X, y)\n",
    "    probs = np.array(model.predict(X))\n",
    "    print('method {:s}, max deviation from true probs {:.3f}'.format(name, np.max(true_probs - probs)))\n",
    "    print('truth'); print_probs(true_probs[0])\n",
    "    print('pred'); print_probs(probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method LBFGS, max deviation from true probs 0.002\n",
      "truth\n",
      "['0.213', '0.030', '0.100', '0.043', '0.017', '0.049', '0.065', '0.085', '0.002', '0.396']\n",
      "pred\n",
      "['0.215', '0.030', '0.100', '0.043', '0.017', '0.048', '0.066', '0.086', '0.002', '0.394']\n"
     ]
    }
   ],
   "source": [
    "compare_skax_method(\"LBFGS\", \"LBFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method armijo, max deviation from true probs 0.105\n",
      "truth\n",
      "['0.213', '0.030', '0.100', '0.043', '0.017', '0.049', '0.065', '0.085', '0.002', '0.396']\n",
      "pred\n",
      "['0.250', '0.023', '0.098', '0.052', '0.015', '0.050', '0.072', '0.067', '0.003', '0.371']\n"
     ]
    }
   ],
   "source": [
    "compare_skax_method(\"armijo\", \"armijo\", batch_size=ntrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "method armijo, max deviation from true probs 0.385\n",
      "truth\n",
      "['0.213', '0.030', '0.100', '0.043', '0.017', '0.049', '0.065', '0.085', '0.002', '0.396']\n",
      "pred\n",
      "['0.124', '0.021', '0.082', '0.056', '0.015', '0.041', '0.061', '0.100', '0.001', '0.499']\n"
     ]
    }
   ],
   "source": [
    "compare_skax_method(\"armijo\", \"armijo\", batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0, train loss 60.227\n",
      "epoch 5, train loss 57.272\n",
      "epoch 10, train loss 55.068\n",
      "epoch 15, train loss 53.719\n",
      "epoch 20, train loss 52.199\n",
      "epoch 25, train loss 51.313\n",
      "epoch 30, train loss 50.806\n",
      "epoch 35, train loss 50.197\n",
      "epoch 40, train loss 49.542\n",
      "epoch 45, train loss 49.370\n",
      "method adam, max deviation from true probs 0.376\n",
      "truth\n",
      "['0.213', '0.030', '0.100', '0.043', '0.017', '0.049', '0.065', '0.085', '0.002', '0.396']\n",
      "pred\n",
      "['0.140', '0.039', '0.116', '0.067', '0.029', '0.083', '0.139', '0.093', '0.009', '0.285']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_skax_method(optax.adam(1e-3), name=\"adam\", batch_size=32, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "750\n",
      "epoch 0, train loss 58.996\n",
      "epoch 5, train loss 51.444\n",
      "epoch 10, train loss 52.099\n",
      "epoch 15, train loss 51.191\n",
      "epoch 20, train loss 50.563\n",
      "epoch 25, train loss 49.535\n",
      "epoch 30, train loss 49.381\n",
      "epoch 35, train loss 48.599\n",
      "epoch 40, train loss 48.014\n",
      "epoch 45, train loss 48.174\n",
      "method adam with warmup, bs=32, max deviation from true probs 0.038\n",
      "truth\n",
      "['0.213', '0.030', '0.100', '0.043', '0.017', '0.049', '0.065', '0.085', '0.002', '0.396']\n",
      "pred\n",
      "['0.208', '0.030', '0.105', '0.043', '0.017', '0.046', '0.063', '0.076', '0.002', '0.409']\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "nepochs = 50\n",
    "total_steps = nepochs*(ntrain//batch_size) \n",
    "print(total_steps)\n",
    "warmup_cosine_decay_scheduler = optax.warmup_cosine_decay_schedule(init_value=1e-3, peak_value=1e-1,\n",
    "                                                                   warmup_steps=int(total_steps*0.1),\n",
    "                                                                   decay_steps=total_steps, end_value=1e-3)\n",
    "optimizer = optax.adam(learning_rate=warmup_cosine_decay_scheduler)\n",
    "compare_skax_method(optimizer, name=\"adam with warmup, bs=32\", batch_size=batch_size, num_epochs=nepochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets with no hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8.617e-01 1.380e-01 2.624e-04]\n",
      "1.0000000000000002\n"
     ]
    }
   ],
   "source": [
    "def fit_classifier_sklearn(key, X, Y):\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', LogisticRegression(random_state=0, max_iter=100, C=1e5))])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "clf = fit_classifier_sklearn(key, X, y)\n",
    "\n",
    "true_probs_pipeline = clf.predict_proba(X)\n",
    "print(true_probs_pipeline[0])\n",
    "print(np.sum(true_probs_pipeline[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max deviation from true probs 0.004\n",
      "truth:  [8.617e-01 1.380e-01 2.624e-04]\n",
      "pred:  [8.622e-01 1.375e-01 2.553e-04]\n"
     ]
    }
   ],
   "source": [
    "def fit_classifier_skax_mlp(key, X, Y):\n",
    "    ndim = X.shape[1]\n",
    "    nclasses  = len(np.unique(Y))\n",
    "    network = MLPNetwork((nclasses,))\n",
    "    model = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, max_iter=500)\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', model)])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "model = fit_classifier_skax_mlp(key, X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "print('max deviation from true probs {:.3f}'.format(np.max(true_probs_pipeline - probs)))\n",
    "print('truth: ', true_probs_pipeline[0])\n",
    "print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ninputs  4\n",
      "method LBFGS, max deviation from true probs 0.011\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.855 0.137 0.009]\n"
     ]
    }
   ],
   "source": [
    "compare_mlp_method(\"LBFGS\", \"LBFGS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit ninputs  4\n",
      "method armijo, bs=32, max deviation from true probs 0.251\n",
      "truth:  [0.858 0.134 0.008]\n",
      "pred:  [0.757 0.199 0.044]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "compare_skax_method(\"armijo\", name=\"armijo, bs=32\", batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural nets with hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kpmurphy/mambaforge/lib/python3.10/site-packages/distrax/_src/utils/conversion.py:143: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in astype is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
      "  return x.astype(jnp.float_)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max deviation from true probs 0.934\n",
      "truth:  [8.617e-01 1.380e-01 2.624e-04]\n",
      "pred:  [1.00e+00 8.77e-26 0.00e+00]\n"
     ]
    }
   ],
   "source": [
    "def fit_classifier_skax_mlp(key, X, Y):\n",
    "    ndim = X.shape[1]\n",
    "    nclasses  = len(np.unique(Y))\n",
    "    network = MLPNetwork((5, nclasses,))\n",
    "    model = NeuralNetClassifier(network, key, nclasses, l2reg=1e-5, max_iter=500)\n",
    "    classifier = Pipeline([\n",
    "            ('standardscaler', StandardScaler()),\n",
    "            ('poly', PolynomialFeatures(degree=2)), \n",
    "            ('logreg', model)])\n",
    "    classifier.fit(np.array(X), np.array(Y))\n",
    "    return classifier\n",
    "    \n",
    "key = jr.PRNGKey(0)\n",
    "model = fit_classifier_skax_mlp(key, X, y)\n",
    "probs = np.array(model.predict(X))\n",
    "print('max deviation from true probs {:.3f}'.format(np.max(true_probs_pipeline - probs)))\n",
    "print('truth: ', true_probs[0])\n",
    "print('pred: ', probs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7e1aafb1a5b8a6c5cc9d9564fe8ce376ad7cec1976d94f450e8b79a35770c931"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
